{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Homework for the course Intro to pattern recognition\n",
    "\n",
    "## Experiments were conducted using python inside a jupyter notebook\n",
    "- Required libs\n",
    "    * numpy\n",
    "    * pandas\n",
    "    * sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from Datasets.dataA import generate_data\n",
    "from Datasets.dataB import generate_chess_data\n",
    "from Datasets.dataC import get_iris_data\n",
    "from Datasets.dataD import get_bumps_data\n",
    "from Datasets.dataE import get_letter_data\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt #Libraries for visualization\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "scaler = MinMaxScaler()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset A\n",
    "\n",
    "- 2 class problem described with the function y = sin(6x)/6 + 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered\n",
      "Finished\n",
      "entered\n",
      "Finished\n",
      "entered\n",
      "Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Za m-voto A dobiv sintetichki generirani 3 m-va so razlichni golemini 150 (podeleno 50%-50%), 1500 i 20000 (dvete 0.3 za test).\\n    Klasite vo odnos na delbenata prava gi narekov \"U\" (up) i \"D\" (down) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataA_1 = generate_data(150)\n",
    "dataA1 = [el[:-1] for el in dataA_1]\n",
    "dataA1c = [el[-1] for el in dataA_1]\n",
    "dataA_2 = generate_data(1500)\n",
    "dataA2 = [el[:-1] for el in dataA_2]\n",
    "dataA2c = [el[-1] for el in dataA_2]\n",
    "dataA_3 = generate_data(20000)\n",
    "dataA3 = [el[:-1] for el in dataA_3]\n",
    "dataA3c = [el[-1] for el in dataA_3]\n",
    "A1_train, A1_test, A1y_train, A1y_test = train_test_split(dataA1,dataA1c, test_size=0.5, random_state=42)\n",
    "# print(len(A1_train))\n",
    "# print(len(A1y_train))\n",
    "A2_train, A2_test, A2y_train, A2y_test = train_test_split(dataA2,dataA2c, test_size=0.3, random_state=42)\n",
    "A3_train, A3_test, A3y_train, A3y_test = train_test_split(dataA3,dataA3c, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Za m-voto A dobiv sintetichki generirani 3 m-va so razlichni golemini 150 (podeleno 50%-50%), 1500 i 20000 (dvete 0.3 za test). Klasite vo odnos na delbenata prava gi narekov \"U\" (up) i \"D\" (down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on dataset A (sintetic 1 - function)\n",
    "\n",
    "## Feed-forward neural network\n",
    "## multilayer perceptron 3 different experiments + AI NN with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurr of A1 NN1 is : 0.9466666666666667\n",
      "Acurr of A1 NN2 is : 0.9466666666666667\n",
      "Acurr of A1 NN3 is : 0.9466666666666667\n",
      "Acurr of A1 NN4 is : 0.9466666666666667\n",
      "Acurr of A1 NN5 is : 0.5866666666666667\n",
      "Acurr of A2 NN1 is : 0.6177777777777778\n",
      "Acurr of A2 NN2 is : 0.6177777777777778\n",
      "Acurr of A2 NN3 is : 0.9288888888888889\n",
      "Acurr of A2 NN4 is : 0.9266666666666666\n",
      "Acurr of A2 NN5 is : 0.9333333333333333\n",
      "Acurr of A3 NN1 is : 0.9453333333333334\n",
      "Acurr of A3 NN2 is : 0.941\n",
      "Acurr of A3 NN3 is : 0.9433333333333334\n",
      "Acurr of A3 NN4 is : 0.5981666666666666\n",
      "Acurr of A3 NN5 is : 0.9381666666666667\n"
     ]
    }
   ],
   "source": [
    "A1_MLP1 = MLPClassifier((1,),learning_rate_init=0.05)\n",
    "A1_MLP2 = MLPClassifier((2,),learning_rate_init=0.05)\n",
    "A1_MLP3 = MLPClassifier((5,),learning_rate_init=0.05)\n",
    "A1_MLP4 = MLPClassifier((2,2),learning_rate_init=0.05)\n",
    "A1_MLP5 = MLPClassifier((10,2),learning_rate_init=0.05)\n",
    "# 50 in hidden LR = 0.01, test size = 0.5*N\n",
    "A1_MLP1.fit(A1_train,A1y_train)\n",
    "predictions_A1 = A1_MLP1.predict(A1_test)\n",
    "N = len(dataA1)\n",
    "n_test = len(A1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A1, A1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A1 NN1 is :\",acc/n_test)\n",
    "# 100 in hidden LR = 0.01, test size = 0.3*N\n",
    "A1_MLP2.fit(A1_train,A1y_train)\n",
    "predictions_A1 = A1_MLP2.predict(A1_test)\n",
    "N = len(dataA1)\n",
    "n_test = len(A1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A1, A1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A1 NN2 is :\",acc/n_test)\n",
    "# 150 in hidden LR = 0.01, test size = 0.3*N\n",
    "A1_MLP3.fit(A1_train,A1y_train)\n",
    "predictions_A1 = A1_MLP3.predict(A1_test)\n",
    "N = len(dataA1)\n",
    "n_test = len(A1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A1, A1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A1 NN3 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "A1_MLP4.fit(A1_train,A1y_train)\n",
    "predictions_A1 = A1_MLP4.predict(A1_test)\n",
    "N = len(dataA1)\n",
    "n_test = len(A1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A1, A1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A1 NN4 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "A1_MLP5.fit(A1_train,A1y_train)\n",
    "predictions_A1 = A1_MLP5.predict(A1_test)\n",
    "N = len(dataA1)\n",
    "n_test = len(A1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A1, A1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A1 NN5 is :\",acc/n_test)\n",
    "# A2\n",
    "A2_MLP1 = MLPClassifier((1,),learning_rate_init=0.05)\n",
    "A2_MLP2 = MLPClassifier((2,),learning_rate_init=0.05)\n",
    "A2_MLP3 = MLPClassifier((5,),learning_rate_init=0.05)\n",
    "A2_MLP4 = MLPClassifier((2,2),learning_rate_init=0.05)\n",
    "A2_MLP5 = MLPClassifier((10,2),learning_rate_init=0.05)\n",
    "# 50 in hidden LR = 0.01, test size = 0.5*N\n",
    "A2_MLP1.fit(A2_train,A2y_train)\n",
    "predictions_A2 = A2_MLP1.predict(A2_test)\n",
    "N = len(dataA2)\n",
    "n_test = len(A2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A2, A2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A2 NN1 is :\",acc/n_test)\n",
    "# 100 in hidden LR = 0.01, test size = 0.3*N\n",
    "A2_MLP2.fit(A2_train,A2y_train)\n",
    "predictions_A2 = A2_MLP2.predict(A2_test)\n",
    "N = len(dataA2)\n",
    "n_test = len(A2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A2, A2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A2 NN2 is :\",acc/n_test)\n",
    "# 150 in hidden LR = 0.01, test size = 0.3*N\n",
    "A2_MLP3.fit(A2_train,A2y_train)\n",
    "predictions_A2 = A2_MLP3.predict(A2_test)\n",
    "N = len(dataA2)\n",
    "n_test = len(A2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A2, A2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A2 NN3 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "A2_MLP4.fit(A2_train,A2y_train)\n",
    "predictions_A2 = A2_MLP4.predict(A2_test)\n",
    "N = len(dataA2)\n",
    "n_test = len(A2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A2, A2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A2 NN4 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "A2_MLP5.fit(A2_train,A2y_train)\n",
    "predictions_A2 = A2_MLP5.predict(A2_test)\n",
    "N = len(dataA2)\n",
    "n_test = len(A2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A2, A2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A2 NN5 is :\",acc/n_test)\n",
    "# A3 hidden floor(n_features * 2/3 + 1)\n",
    "A3_MLP1 = MLPClassifier((1,),learning_rate_init=0.05)\n",
    "A3_MLP2 = MLPClassifier((2,),learning_rate_init=0.05)\n",
    "A3_MLP3 = MLPClassifier((5,),learning_rate_init=0.05)\n",
    "A3_MLP4 = MLPClassifier((2,2),learning_rate_init=0.05)\n",
    "A3_MLP5 = MLPClassifier((10,2),learning_rate_init=0.05)\n",
    "# 50 in hidden LR = 0.01, test size = 0.5*N\n",
    "A3_MLP1.fit(A3_train,A3y_train)\n",
    "predictions_A3 = A3_MLP1.predict(A3_test)\n",
    "N = len(dataA3)\n",
    "n_test = len(A3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A3, A3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A3 NN1 is :\",acc/n_test)\n",
    "# 100 in hidden LR = 0.01, test size = 0.3*N\n",
    "A3_MLP2.fit(A3_train,A3y_train)\n",
    "predictions_A3 = A3_MLP2.predict(A3_test)\n",
    "N = len(dataA3)\n",
    "n_test = len(A3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A3, A3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A3 NN2 is :\",acc/n_test)\n",
    "# 150 in hidden LR = 0.01, test size = 0.3*N\n",
    "A3_MLP3.fit(A3_train,A3y_train)\n",
    "predictions_A3 = A3_MLP3.predict(A3_test)\n",
    "N = len(dataA3)\n",
    "n_test = len(A3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A3, A3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A3 NN3 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "A3_MLP4.fit(A3_train,A3y_train)\n",
    "predictions_A3 = A3_MLP4.predict(A3_test)\n",
    "N = len(dataA3)\n",
    "n_test = len(A3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A3, A3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A3 NN4 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "A3_MLP5.fit(A3_train,A3y_train)\n",
    "predictions_A3 = A3_MLP5.predict(A3_test)\n",
    "N = len(dataA3)\n",
    "n_test = len(A3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_A3, A3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of A3 NN5 is :\",acc/n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM linear and Gausian done with default 5-fold cross validation on the x% we are providing for training. Then I am doing the tests with the rest of the y% that i left for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for A1: {'C': 1, 'kernel': 'linear'}\n",
      "Report for development parameters\n",
      "0.961 (+/-0.065) for {'C': 1, 'kernel': 'linear'}\n",
      "0.953 (+/-0.078) for {'C': 10, 'kernel': 'linear'}\n",
      "0.953 (+/-0.078) for {'C': 100, 'kernel': 'linear'}\n",
      "0.953 (+/-0.078) for {'C': 1000, 'kernel': 'linear'}\n",
      "0.287 (+/-0.033) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.287 (+/-0.033) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.287 (+/-0.033) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.287 (+/-0.033) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.287 (+/-0.033) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.287 (+/-0.033) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.953 (+/-0.078) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.953 (+/-0.078) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.287 (+/-0.033) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.961 (+/-0.065) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.961 (+/-0.065) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.953 (+/-0.078) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "Best parameters for A2: {'C': 1000, 'kernel': 'linear'}\n",
      "Best parameters for A3: {'C': 1000, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['rbf'], 'gamma': [0.001,1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "# A1\n",
    "clf1 = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro')\n",
    "clf2 = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro')\n",
    "clf3 = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro')\n",
    "clf1.fit(A1_train, A1y_train)\n",
    "print(\"Best parameters for A1:\", clf1.best_params_)\n",
    "# Full grid only for A1\n",
    "print(\"Report for development parameters\")\n",
    "means = clf1.cv_results_['mean_test_score']\n",
    "stds = clf1.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf1.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "y1_true, y1_pred = A1y_test, clf1.predict(A1_test)\n",
    "\n",
    "\n",
    "# A2\n",
    "clf2.fit(A2_train, A2y_train)\n",
    "print(\"Best parameters for A2:\", clf2.best_params_)\n",
    "y2_true, y2_pred = A2y_test, clf2.predict(A2_test)\n",
    "\n",
    "# A3\n",
    "clf3.fit(A3_train, A3y_train)\n",
    "y3_true, y3_pred = A3y_test, clf3.predict(A3_test)\n",
    "print(\"Best parameters for A3:\", clf3.best_params_)\n",
    "# Trgnuvajkji od faktot deka klasifikacijata na tochki kaj ovoj model moze da se podeli so edna prava\n",
    "# ni go potvrduva dobieniot rezultat koj ni kazuva deka najdobar model na SVM tuka e linearniot so C = 1000\n",
    "# bez razlika na goleminata na trening mnozestvoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM report\n",
    "- Here we have the results of the SVM classification\n",
    "- WIth precision we are showing the number of True positives for each of the classes against the total number of positives\n",
    "* tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for A1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           d       0.95      0.95      0.95        44\n",
      "           u       0.94      0.94      0.94        31\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.95      0.95      0.95        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n",
      "\n",
      "Classification report for A2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           d       0.93      0.96      0.94       278\n",
      "           u       0.93      0.89      0.91       172\n",
      "\n",
      "    accuracy                           0.93       450\n",
      "   macro avg       0.93      0.92      0.93       450\n",
      "weighted avg       0.93      0.93      0.93       450\n",
      "\n",
      "\n",
      "Classification report for A3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           d       0.96      0.95      0.95      3589\n",
      "           u       0.93      0.94      0.93      2411\n",
      "\n",
      "    accuracy                           0.95      6000\n",
      "   macro avg       0.94      0.94      0.94      6000\n",
      "weighted avg       0.95      0.95      0.95      6000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Od rezultatite uvidov deka za m-voto A najdobri klasifikatori se linearen SVM i nevronska mreza so 2 vo skrien sloj,\\nod koi smetam deka SVM e pobrz pa zatoa i podobar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Classification report for A1:\")\n",
    "print(classification_report(y1_true, y1_pred))\n",
    "print()\n",
    "print(\"Classification report for A2:\")\n",
    "print(classification_report(y2_true, y2_pred))\n",
    "print()\n",
    "print(\"Classification report for A3:\")\n",
    "print(classification_report(y3_true, y3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the results best classifiers for the set A are linear SVM and NN with 2 neurons in the hidden layer.\n",
    "\n",
    "# Dataset B - chess table\n",
    "\n",
    "### Taking 3 different instances and doing the splits as stated in the requirements.\n",
    "- 1st instance 1/2 for testing 1/2 for validation\n",
    "- 2nd and 3rd instances 70% for training 30% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n"
     ]
    }
   ],
   "source": [
    "dataB_1 = generate_chess_data(150)\n",
    "dataB_2 = generate_chess_data(1500)\n",
    "dataB_3 = generate_chess_data(20000)\n",
    "dataB1 = [el[:-1] for el in dataB_1]\n",
    "dataB1c = [el[-1] for el in dataB_1]\n",
    "dataB2 = [el[:-1] for el in dataB_2]\n",
    "dataB2c = [el[-1] for el in dataB_2]\n",
    "dataB3 = [el[:-1] for el in dataB_3]\n",
    "dataB3c = [el[-1] for el in dataB_3]\n",
    "B1_train, B1_test, B1y_train, B1y_test = train_test_split(dataB1,dataB1c, test_size=0.5, random_state=42)\n",
    "B2_train, B2_test, B2y_train, B2y_test = train_test_split(dataB2,dataB2c, test_size=0.3, random_state=42)\n",
    "B3_train, B3_test, B3y_train, B3y_test = train_test_split(dataB3,dataB3c, test_size=0.3, random_state=42)\n",
    "print(len(B3_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on dataset B (CHESS)\n",
    "\n",
    "### Multilayer perceptron 3 different experiments + NN with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurr of B1 NN1 is : 0.5466666666666666\n",
      "Acurr of B1 NN2 is : 0.5866666666666667\n",
      "Acurr of B1 NN3 is : 0.5866666666666667\n",
      "Acurr of B1 NN4 is : 0.48\n",
      "Acurr of B1 NN5 is : 0.6\n",
      "Acurr of B2 NN1 is : 0.5533333333333333\n",
      "Acurr of B2 NN2 is : 0.66\n",
      "Acurr of B2 NN3 is : 0.7333333333333333\n",
      "Acurr of B2 NN4 is : 0.44666666666666666\n",
      "Acurr of B2 NN5 is : 0.6955555555555556\n",
      "Acurr of B3 NN1 is : 0.498\n",
      "Acurr of B3 NN2 is : 0.9786666666666667\n",
      "Acurr of B3 NN3 is : 0.9721666666666666\n",
      "Acurr of B3 NN4 is : 0.8405\n",
      "Acurr of B3 NN5 is : 0.9623333333333334\n"
     ]
    }
   ],
   "source": [
    "B1_MLP1 = MLPClassifier((2,),learning_rate_init=0.01)\n",
    "B1_MLP2 = MLPClassifier((500,),learning_rate_init=0.01)\n",
    "B1_MLP3 = MLPClassifier((1000,),learning_rate_init=0.01)\n",
    "B1_MLP4 = MLPClassifier((50,2),learning_rate_init=0.01)\n",
    "B1_MLP5 = MLPClassifier((100,2),learning_rate_init=0.01)\n",
    "# 50 in hidden LR = 0.01, test size = 0.5*N\n",
    "B1_MLP1.fit(B1_train,B1y_train)\n",
    "predictions_B1 = B1_MLP1.predict(B1_test)\n",
    "N = len(dataB1)\n",
    "n_test = len(B1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B1, B1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B1 NN1 is :\",acc/n_test)\n",
    "# 100 in hidden LR = 0.01, test size = 0.3*N\n",
    "B1_MLP2.fit(B1_train,B1y_train)\n",
    "predictions_B1 = B1_MLP2.predict(B1_test)\n",
    "N = len(dataB1)\n",
    "n_test = len(B1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B1, B1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B1 NN2 is :\",acc/n_test)\n",
    "# 150 in hidden LR = 0.01, test size = 0.3*N\n",
    "B1_MLP3.fit(B1_train,B1y_train)\n",
    "predictions_B = B1_MLP3.predict(B1_test)\n",
    "N = len(dataB1)\n",
    "n_test = len(B1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B1, B1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B1 NN3 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "B1_MLP4.fit(B1_train,B1y_train)\n",
    "predictions_B1 = B1_MLP4.predict(B1_test)\n",
    "N = len(dataB1)\n",
    "n_test = len(B1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B1, B1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B1 NN4 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "B1_MLP5.fit(B1_train,B1y_train)\n",
    "predictions_B1 = B1_MLP5.predict(B1_test)\n",
    "N = len(dataB1)\n",
    "n_test = len(B1_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B1, B1y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B1 NN5 is :\",acc/n_test)\n",
    "# B2\n",
    "B2_MLP1 = MLPClassifier((2,),learning_rate_init=0.05)\n",
    "B2_MLP2 = MLPClassifier((500,),learning_rate_init=0.05)\n",
    "B2_MLP3 = MLPClassifier((1000,),learning_rate_init=0.05)\n",
    "B2_MLP4 = MLPClassifier((50,2),learning_rate_init=0.05)\n",
    "B2_MLP5 = MLPClassifier((100,2),learning_rate_init=0.05)\n",
    "# 50 in hidden LR = 0.01, test size = 0.5*N\n",
    "B2_MLP1.fit(B2_train,B2y_train)\n",
    "predictions_B2 = B2_MLP1.predict(B2_test)\n",
    "N = len(dataB2)\n",
    "n_test = len(B2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B2, B2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B2 NN1 is :\",acc/n_test)\n",
    "# 100 in hidden LR = 0.01, test size = 0.3*N\n",
    "B2_MLP2.fit(B2_train,B2y_train)\n",
    "predictions_B2 = B2_MLP2.predict(B2_test)\n",
    "N = len(dataB2)\n",
    "n_test = len(B2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B2, B2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B2 NN2 is :\",acc/n_test)\n",
    "# 150 in hidden LR = 0.01, test size = 0.3*N\n",
    "B2_MLP3.fit(B2_train,B2y_train)\n",
    "predictions_B2 = B2_MLP3.predict(B2_test)\n",
    "N = len(dataB2)\n",
    "n_test = len(B2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B2, B2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B2 NN3 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "B2_MLP4.fit(B2_train,B2y_train)\n",
    "predictions_B2 = B2_MLP4.predict(B2_test)\n",
    "N = len(dataB2)\n",
    "n_test = len(B2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B2, B2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B2 NN4 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "B2_MLP5.fit(B2_train,B2y_train)\n",
    "predictions_B2 = B2_MLP5.predict(B2_test)\n",
    "N = len(dataB2)\n",
    "n_test = len(B2_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B2, B2y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B2 NN5 is :\",acc/n_test)\n",
    "# B3 hidden floor(n_features * 2/3 + 1)\n",
    "B3_MLP1 = MLPClassifier((2,),learning_rate_init=0.01)\n",
    "B3_MLP2 = MLPClassifier((500,),learning_rate_init=0.01)\n",
    "B3_MLP3 = MLPClassifier((1000,),learning_rate_init=0.01)\n",
    "B3_MLP4 = MLPClassifier((50,2),learning_rate_init=0.01)\n",
    "B3_MLP5 = MLPClassifier((100,2),learning_rate_init=0.01)\n",
    "# 50 in hidden LR = 0.01, test size = 0.5*N\n",
    "B3_MLP1.fit(B3_train,B3y_train)\n",
    "predictions_B3 = B3_MLP1.predict(B3_test)\n",
    "N = len(dataB3)\n",
    "n_test = len(B3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B3, B3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B3 NN1 is :\",acc/n_test)\n",
    "# 100 in hidden LR = 0.01, test size = 0.3*N\n",
    "B3_MLP2.fit(B3_train,B3y_train)\n",
    "predictions_B3 = B3_MLP2.predict(B3_test)\n",
    "N = len(dataB3)\n",
    "n_test = len(B3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B3, B3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B3 NN2 is :\",acc/n_test)\n",
    "# 150 in hidden LR = 0.01, test size = 0.3*N\n",
    "B3_MLP3.fit(B3_train,B3y_train)\n",
    "predictions_B3 = B3_MLP3.predict(B3_test)\n",
    "N = len(dataB3)\n",
    "n_test = len(B3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B3, B3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B3 NN3 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "B3_MLP4.fit(B3_train,B3y_train)\n",
    "predictions_B3 = B3_MLP4.predict(B3_test)\n",
    "N = len(dataB3)\n",
    "n_test = len(B3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B3, B3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B3 NN4 is :\",acc/n_test)\n",
    "# 100 in 2 hidden LR = 0.01, test size = 0.3*N\n",
    "B3_MLP5.fit(B3_train,B3y_train)\n",
    "predictions_B3 = B3_MLP5.predict(B3_test)\n",
    "N = len(dataB3)\n",
    "n_test = len(B3_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_B3, B3y_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of B3 NN5 is :\",acc/n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM For B\n",
    "\n",
    "### Done with default 5-fold cross validation on the x% we are providing for training. Then I am doing the tests with the rest of the y% that i left for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for B1: {'C': 10, 'kernel': 'linear'}\n",
      "Report for development parameters\n",
      "0.599 (+/-0.202) for {'C': 1, 'kernel': 'linear'}\n",
      "0.612 (+/-0.205) for {'C': 10, 'kernel': 'linear'}\n",
      "0.612 (+/-0.205) for {'C': 100, 'kernel': 'linear'}\n",
      "0.612 (+/-0.205) for {'C': 1000, 'kernel': 'linear'}\n",
      "0.260 (+/-0.027) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.260 (+/-0.027) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.260 (+/-0.027) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.260 (+/-0.027) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.260 (+/-0.027) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.260 (+/-0.027) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.277 (+/-0.079) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.277 (+/-0.079) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.260 (+/-0.027) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.612 (+/-0.205) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.612 (+/-0.205) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.277 (+/-0.079) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "Best parameters for B2: {'C': 1, 'kernel': 'linear'}\n",
      "Best parameters for B3: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Kaj tochkite raspredeleni kako vo shahovska tabla SVM linearen model pretpostavuvav deka ne moze da ni pomogne, no\\n    naidov na faktot deka bash linearniot model e podobar vo odnos na onoj so gausov kernel.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['rbf'], 'gamma': [0.001,1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "# B1\n",
    "clf1 = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro')\n",
    "clf2 = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro')\n",
    "clf3 = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro')\n",
    "clf1.fit(B1_train, B1y_train)\n",
    "print(\"Best parameters for B1:\", clf1.best_params_)\n",
    "# Full grid only for B1\n",
    "print(\"Report for development parameters\")\n",
    "means = clf1.cv_results_['mean_test_score']\n",
    "stds = clf1.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf1.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "y1_true, y1_pred = B1y_test, clf1.predict(B1_test)\n",
    "\n",
    "\n",
    "# B2\n",
    "clf2.fit(B2_train, B2y_train)\n",
    "print(\"Best parameters for B2:\", clf2.best_params_)\n",
    "y2_true, y2_pred = B2y_test, clf2.predict(B2_test)\n",
    "\n",
    "# B3\n",
    "clf3.fit(B3_train, B3y_train)\n",
    "y3_true, y3_pred = B3y_test, clf3.predict(B3_test)\n",
    "print(\"Best parameters for B3:\", clf3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results here are pointing that the linear SVM is better than the one with gaussian kernel on a smaller dataset. And as expected for large non-linear datasets the SVM with gaussian kernel is better.\n",
    "\n",
    "## SVM report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for B1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50        36\n",
      "           1       0.54      0.54      0.54        39\n",
      "\n",
      "    accuracy                           0.52        75\n",
      "   macro avg       0.52      0.52      0.52        75\n",
      "weighted avg       0.52      0.52      0.52        75\n",
      "\n",
      "\n",
      "Classification report for B2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       249\n",
      "           1       0.45      1.00      0.62       201\n",
      "\n",
      "    accuracy                           0.45       450\n",
      "   macro avg       0.22      0.50      0.31       450\n",
      "weighted avg       0.20      0.45      0.28       450\n",
      "\n",
      "\n",
      "Classification report for B3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.51      0.51      3012\n",
      "           1       0.50      0.50      0.50      2988\n",
      "\n",
      "    accuracy                           0.50      6000\n",
      "   macro avg       0.50      0.50      0.50      6000\n",
      "weighted avg       0.50      0.50      0.50      6000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Za da imame dobar model na predviduvanje potrebno ni e pogolemo mnozestvo so cel da imame dobro treniranje na nevronska mreza\\n    koja vo skrien sloj ima 1000 nevroni'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM report\n",
    "print(\"Classification report for B1:\")\n",
    "print(classification_report(y1_true, y1_pred))\n",
    "print()\n",
    "print(\"Classification report for B2:\")\n",
    "print(classification_report(y2_true, y2_pred))\n",
    "print()\n",
    "print(\"Classification report for B3:\")\n",
    "print(classification_report(y3_true, y3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the validation done to the best SVM models that I got from GridSearch, I can state that on this dataset SVM should not be used regarding the fact that on a large number of instances the accuracy is only 50%. Optimisation for this could be choosing some better kernel, but again the results won't be promising.\n",
    "\n",
    "### That's why a good prediction model for this dataset is the MLP that needs a lot of training data and also the best results are shown with 500 neurons in the single hidden layer. This model has 97.8% accuracy.\n",
    "\n",
    "# Iris dataset\n",
    "\n",
    "- 10% used for validation and 90% usef for 10-fold validation searching for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Data processing and Normalization\n",
    "dataIrisFull = np.loadtxt(open('../Datasets/iris.data'),delimiter=',',dtype=np.object)\n",
    "dataIris = dataIrisFull[:,:-1].astype(np.float)\n",
    "X_iris = scaler.fit_transform(dataIris)\n",
    "dataIris_class = dataIrisFull[:,-1].astype(np.str)\n",
    "i_train, i_test, iY_train, iY_test = train_test_split(X_iris,dataIris_class, test_size=0.1, random_state=42)\n",
    "print(len(i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for iris neural network: {'alpha': 0.0001, 'hidden_layer_sizes': (100,)}\n",
      "Report for development parameters:\n",
      "0.834 (+/-0.387) for {'alpha': 0.0001, 'hidden_layer_sizes': (3,)}\n",
      "0.963 (+/-0.119) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,)}\n",
      "0.717 (+/-0.588) for {'alpha': 0.0001, 'hidden_layer_sizes': (4, 2)}\n",
      "0.748 (+/-0.525) for {'alpha': 0.05, 'hidden_layer_sizes': (3,)}\n",
      "0.963 (+/-0.119) for {'alpha': 0.05, 'hidden_layer_sizes': (100,)}\n",
      "0.830 (+/-0.397) for {'alpha': 0.05, 'hidden_layer_sizes': (4, 2)}\n",
      "Classification report for iris neural network:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         6\n",
      "Iris-versicolor       1.00      1.00      1.00         6\n",
      " Iris-virginica       1.00      1.00      1.00         3\n",
      "\n",
      "       accuracy                           1.00        15\n",
      "      macro avg       1.00      1.00      1.00        15\n",
      "   weighted avg       1.00      1.00      1.00        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification on iris dataset\n",
    "# multilayer perceptron neural network 10-fold cross validation\n",
    "# 10% of data used for testing. From the 90% of the data the CV is performed to find the best model\n",
    "\n",
    "iris_MLP = MLPClassifier(learning_rate_init=0.01)\n",
    "tuned_parameters = [{'hidden_layer_sizes': [(3,),(100,),(4,2)],\n",
    "    'alpha': [0.0001, 0.05]}]\n",
    "clf_iris_NN = GridSearchCV(iris_MLP, tuned_parameters, cv=10) # 10-fold cross validation grid search\n",
    "clf_iris_NN.fit(i_train, iY_train) # passed 90% data for features and classes leaving the grid search to do the full 10-fold CV\n",
    "print(\"Best parameters for iris neural network:\", clf_iris_NN.best_params_)\n",
    "print(\"Report for development parameters:\")\n",
    "means = clf_iris_NN.cv_results_['mean_test_score']\n",
    "stds = clf_iris_NN.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf_iris_NN.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "y1_true, y1_pred = iY_test, clf_iris_NN.predict(i_test)\n",
    "print(\"Classification report for iris neural network:\")\n",
    "print(classification_report(y1_true, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM For Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for iris: {'C': 100, 'kernel': 'linear'}\n",
      "Report for development parameters\n",
      "0.961 (+/-0.130) for {'C': 1, 'kernel': 'linear'}\n",
      "0.975 (+/-0.078) for {'C': 10, 'kernel': 'linear'}\n",
      "0.981 (+/-0.076) for {'C': 100, 'kernel': 'linear'}\n",
      "0.975 (+/-0.078) for {'C': 1000, 'kernel': 'linear'}\n",
      "0.116 (+/-0.029) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.116 (+/-0.029) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.116 (+/-0.029) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.504 (+/-0.038) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.504 (+/-0.038) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.116 (+/-0.029) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.955 (+/-0.080) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.955 (+/-0.080) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.506 (+/-0.033) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.977 (+/-0.057) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.977 (+/-0.057) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.955 (+/-0.080) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation CV\n",
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['rbf'], 'gamma': [0.001,1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "clf_iris = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro', cv=10) # 10-fold cross validation grid search\n",
    "clf_iris.fit(i_train, iY_train) # passed 90% data for features and classes leaving the grid search to do the full 10-fold CV\n",
    "print(\"Best parameters for iris:\", clf_iris.best_params_)\n",
    "print(\"Report for development parameters\")\n",
    "means = clf_iris.cv_results_['mean_test_score']\n",
    "stds = clf_iris.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf_iris.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "y1_true, y1_pred = iY_test, clf_iris.predict(i_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM report \n",
    "- Best SVM model is the lienar with C = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for iris:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         6\n",
      "Iris-versicolor       1.00      0.83      0.91         6\n",
      " Iris-virginica       0.75      1.00      0.86         3\n",
      "\n",
      "       accuracy                           0.93        15\n",
      "      macro avg       0.92      0.94      0.92        15\n",
      "   weighted avg       0.95      0.93      0.94        15\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Za da imame dobar model na predviduvanje potrebno ni e '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Tuka imame rezultati od test klasifikacijata\n",
    "    So precision pokazuvame br na True Positives za sekoja od klasite nasproti vk broj na pozitivni tp / (tp + fp)\n",
    "\"\"\"\n",
    "print(\"Classification report for iris:\")\n",
    "print(classification_report(y1_true, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The linear SVM model has accuracy of 93% \n",
    " \n",
    "### The best model for this dataset is the MLP with alpha = 0.001 ( penalty )  and 100 neurons in the single hidden layer. It passed the validation with 100% precision.\n",
    " \n",
    "# Seismic bumps dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    1 ...    0    0    0]\n",
      " [   0    0    1 ...    0 2000 2000]\n",
      " [   0    0    1 ...    0    0    0]\n",
      " ...\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...    0    0    0]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "259\n"
     ]
    }
   ],
   "source": [
    "dataBumpsFull = np.loadtxt(open(\"../Datasets/seismic-bumps.csv\"),delimiter=',',dtype=np.object)\n",
    "dataBumps = dataBumpsFull[:,:-1]\n",
    "dataBumps_class = dataBumpsFull[:,-1].astype(np.int)\n",
    "dataBumps[dataBumps == 'a'] = 0\n",
    "dataBumps[dataBumps == 'b'] = 1\n",
    "dataBumps[dataBumps == 'c'] = 2\n",
    "dataBumps[dataBumps == 'd'] = 3\n",
    "dataBumps[dataBumps == 'W'] = 0\n",
    "dataBumps[dataBumps == 'N'] = 1\n",
    "dataBumps = dataBumps[:].astype(np.int)\n",
    "X_bumps = scaler.fit_transform(dataBumps)\n",
    "print(dataBumps)\n",
    "print(dataBumps_class)\n",
    "bumps_train, bumps_test, bumpsY_train, bumpsY_test = train_test_split(X_bumps,dataBumps_class, test_size=0.1, shuffle=True, random_state=42)\n",
    "print(len(bumps_test))\n",
    "# 10-fold Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on bumps dataset\n",
    "\n",
    "### 10% of data used for testing. From the 90% of the data the 10-fold CV is performed to find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for bumps neural network: {'alpha': 0.05, 'hidden_layer_sizes': (13,)}\n",
      "Report for development parameters:\n",
      "0.934 (+/-0.012) for {'alpha': 0.0001, 'hidden_layer_sizes': (13,)}\n",
      "0.935 (+/-0.014) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,)}\n",
      "0.936 (+/-0.009) for {'alpha': 0.0001, 'hidden_layer_sizes': (8, 2)}\n",
      "0.936 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (13,)}\n",
      "0.936 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100,)}\n",
      "0.936 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (8, 2)}\n",
      "Classification report for bumps neural network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       238\n",
      "           1       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.92       259\n",
      "   macro avg       0.46      0.50      0.48       259\n",
      "weighted avg       0.84      0.92      0.88       259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bumps_MLP = MLPClassifier(learning_rate_init=0.01)\n",
    "tuned_parameters = [{'hidden_layer_sizes': [(13,),(100,),(8,2)],\n",
    "    'alpha': [0.0001, 0.05]}]\n",
    "clf_bumps_NN = GridSearchCV(bumps_MLP, tuned_parameters, cv=10) # 10-fold cross validation grid search\n",
    "clf_bumps_NN.fit(bumps_train, bumpsY_train) # passed 90% data for features and classes leaving the grid search to do the full 10-fold CV\n",
    "print(\"Best parameters for bumps neural network:\", clf_bumps_NN.best_params_)\n",
    "print(\"Report for development parameters:\")\n",
    "means = clf_bumps_NN.cv_results_['mean_test_score']\n",
    "stds = clf_bumps_NN.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf_bumps_NN.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "y1_true, y1_pred = bumpsY_test, clf_bumps_NN.predict(bumps_test)\n",
    "print(\"Classification report for bumps neural network:\")\n",
    "print(classification_report(y1_true, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM For seismic bumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for bumps: {'C': 1, 'kernel': 'linear'}\n",
      "Report for development parameters: \n",
      "0.468 (+/-0.001) for {'C': 1, 'kernel': 'linear'}\n",
      "0.468 (+/-0.001) for {'C': 10, 'kernel': 'linear'}\n",
      "0.468 (+/-0.001) for {'C': 100, 'kernel': 'linear'}\n",
      "0.468 (+/-0.001) for {'C': 1000, 'kernel': 'linear'}\n",
      "0.468 (+/-0.001) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.468 (+/-0.001) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation CV\n",
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['rbf'], 'gamma': [0.001,1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro', cv=10)\n",
    "clf.fit(bumps_train, bumpsY_train)\n",
    "print(\"Best parameters for bumps:\", clf.best_params_)\n",
    "print(\"Report for development parameters: \")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "y1_true, y1_pred = bumpsY_test, clf.predict(bumps_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM report\n",
    "\n",
    "- Here we can see that with the linear SVM with C = 1 we are having 92% accuracy on validation set, which is the same with the best NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for bumps:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       238\n",
      "           1       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.92       259\n",
      "   macro avg       0.46      0.50      0.48       259\n",
      "weighted avg       0.84      0.92      0.88       259\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Za da imame dobar model na predviduvanje potrebno ni e'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" Tuka imame rezultati od test klasifikacijata\n",
    "    So precision pokazuvame br na True Positives za sekoja od klasite nasproti vk broj na pozitivni tp / (tp + fp)\n",
    "    So recall pokazuvame \"\"\"\n",
    "print(\"Classification report for bumps:\")\n",
    "print(classification_report(y1_true, y1_pred))\n",
    "\"\"\"  Za da imame dobar model na predviduvanje potrebno ni e\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letter recognition dataset\n",
    "\n",
    "## Letter recognition data proccesing and no need for extra Normalization, the dataset was already Normalized to integer values 0-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6666\n"
     ]
    }
   ],
   "source": [
    "dataLettersFull = np.loadtxt(open('../Datasets/letter-recognition.data'),delimiter=',',dtype=np.object)\n",
    "dataLetters = dataLettersFull[:,1:].astype(np.int)\n",
    "dataLetters_class = dataLettersFull[:,0].astype(np.str)\n",
    "let_train, let_test, letY_train, letY_test = train_test_split(dataLetters,dataLetters_class, test_size=0.3333, random_state=42)\n",
    "print(len(let_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on letter dataset\n",
    "- Data is seperated into 1/3 for validation and 2/3 for training the models\n",
    "- From the results below we can see that the best NN model is the one with 700 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurr of letters dataset NN1 is : 0.5876087608760876\n",
      "Acurr of letters dataset NN2 is : 0.8964896489648965\n",
      "Acurr of letters dataset NN3 is : 0.9273927392739274\n",
      "Acurr of letters dataset NN4 is : 0.15946594659465946\n",
      "Acurr of letters dataset NN5 is : 0.32268226822682267\n"
     ]
    }
   ],
   "source": [
    "letters_MLP1 = MLPClassifier((6,),learning_rate_init=0.01)\n",
    "letters_MLP2 = MLPClassifier((100,),learning_rate_init=0.01)\n",
    "letters_MLP3 = MLPClassifier((700,),learning_rate_init=0.01)\n",
    "letters_MLP4 = MLPClassifier((2,2),learning_rate_init=0.01)\n",
    "letters_MLP5 = MLPClassifier((4,2),learning_rate_init=0.01)\n",
    "# 50 in hidden LR = 0.01, test size = 0.5*N\n",
    "letters_MLP1.fit(let_train,letY_train)\n",
    "predictions_let = letters_MLP1.predict(let_test)\n",
    "N = len(dataLetters)\n",
    "n_test = len(let_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_let, letY_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of letters dataset NN1 is :\",acc/n_test)\n",
    "letters_MLP2.fit(let_train,letY_train)\n",
    "predictions_let = letters_MLP2.predict(let_test)\n",
    "N = len(dataLetters)\n",
    "n_test = len(let_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_let, letY_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of letters dataset NN2 is :\",acc/n_test)\n",
    "letters_MLP3.fit(let_train,letY_train)\n",
    "predictions_let = letters_MLP3.predict(let_test)\n",
    "N = len(dataLetters)\n",
    "n_test = len(let_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_let, letY_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of letters dataset NN3 is :\",acc/n_test)\n",
    "letters_MLP4.fit(let_train,letY_train)\n",
    "predictions_let = letters_MLP4.predict(let_test)\n",
    "N = len(dataLetters)\n",
    "n_test = len(let_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_let, letY_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of letters dataset NN4 is :\",acc/n_test)\n",
    "letters_MLP5.fit(let_train,letY_train)\n",
    "predictions_let = letters_MLP5.predict(let_test)\n",
    "N = len(dataLetters)\n",
    "n_test = len(let_test)\n",
    "acc = 0\n",
    "for pred, ver in zip(predictions_let, letY_test):\n",
    "    if pred == ver:\n",
    "       acc += 1\n",
    "print(\"Acurr of letters dataset NN5 is :\",acc/n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM For Letters\n",
    "\n",
    "- As the best SVM model is choosen the one with gaussian kernel and parameters C = 1000 and gamma = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for letters: {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Report for development parameters:\n",
      "0.851 (+/-0.002) for {'C': 1, 'kernel': 'linear'}\n",
      "0.848 (+/-0.007) for {'C': 10, 'kernel': 'linear'}\n",
      "0.847 (+/-0.009) for {'C': 100, 'kernel': 'linear'}\n",
      "0.847 (+/-0.008) for {'C': 1000, 'kernel': 'linear'}\n",
      "0.813 (+/-0.008) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.813 (+/-0.008) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.635 (+/-0.022) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.883 (+/-0.011) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.883 (+/-0.011) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.800 (+/-0.012) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.923 (+/-0.007) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.923 (+/-0.007) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.849 (+/-0.008) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.944 (+/-0.006) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.944 (+/-0.006) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.876 (+/-0.008) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Kaj letter dataset najdobar model spored rezulatite od GridSearch e '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the parameters by cross-validation CV\n",
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['rbf'], 'gamma': [0.001,1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "# letters 1\n",
    "clf_let = GridSearchCV(SVC(), tuned_parameters, scoring='precision_macro') #  5-fold CV on 2/3 of data\n",
    "clf_let.fit(let_train, letY_train)\n",
    "print(\"Best parameters for letters:\", clf_let.best_params_)\n",
    "print(\"Report for development parameters:\")\n",
    "means = clf_let.cv_results_['mean_test_score']\n",
    "stds = clf_let.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf_let.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "y1_true, y1_pred = letY_test, clf_let.predict(let_test)\n",
    "\n",
    "\"\"\" Kaj letter dataset najdobar model spored rezulatite od GridSearch e \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM report\n",
    "\n",
    "### The best SVM model has proven to be 96% precise which makes the best model for this datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for letters:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.98      0.99      0.98       266\n",
      "           B       0.92      0.95      0.94       256\n",
      "           C       0.91      0.91      0.91       214\n",
      "           D       0.91      0.96      0.94       284\n",
      "           E       0.94      0.95      0.94       261\n",
      "           F       0.92      0.98      0.95       235\n",
      "           G       0.92      0.94      0.93       247\n",
      "           H       0.94      0.84      0.89       241\n",
      "           I       0.96      0.96      0.96       244\n",
      "           J       0.97      0.94      0.95       248\n",
      "           K       0.91      0.94      0.92       204\n",
      "           L       0.98      0.95      0.96       249\n",
      "           M       0.99      0.96      0.97       276\n",
      "           N       0.97      0.94      0.95       262\n",
      "           O       0.94      0.93      0.94       251\n",
      "           P       0.99      0.96      0.97       280\n",
      "           Q       0.96      0.96      0.96       281\n",
      "           R       0.92      0.94      0.93       256\n",
      "           S       0.99      0.97      0.98       255\n",
      "           T       0.97      0.96      0.97       260\n",
      "           U       0.98      0.98      0.98       299\n",
      "           V       0.97      0.97      0.97       269\n",
      "           W       0.96      0.98      0.97       240\n",
      "           X       0.96      0.98      0.97       275\n",
      "           Y       0.97      0.98      0.97       273\n",
      "           Z       0.98      0.98      0.98       240\n",
      "\n",
      "    accuracy                           0.96      6666\n",
      "   macro avg       0.95      0.95      0.95      6666\n",
      "weighted avg       0.96      0.96      0.96      6666\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Za da imame dobar model na predviduvanje potrebno ni e '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Tuka imame rezultati od test klasifikacijata so 1/3 od data\n",
    "    So precision pokazuvame br na True Positives za sekoja od klasite nasproti vk broj na pozitivni tp / (tp + fp)\n",
    "\"\"\"\n",
    "print(\"Classification report for letters:\")\n",
    "print(classification_report(y1_true, y1_pred))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
